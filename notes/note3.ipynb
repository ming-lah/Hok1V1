{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8baaf22d",
   "metadata": {},
   "source": [
    "### **网络结构**\n",
    "\n",
    "\n",
    "原本的模型网络结构`feature_vec (in_dim) → concat_mlp(256) → 6个策略头 + 价值头`\n",
    "\n",
    "当前的结构：在`mlp`后面加入自注意力模块，将其拆开成`T=4`的虚拟`token`，每个维度`D=64`，通过两层`Transformer Encoder`。对`token`进行平均池化，最后汇聚回256维和原来的256维做残差融合+LayerNorm\n",
    "\n",
    "加入自注意力机制后的网络结构：\\\n",
    "`feature_vec (in_dim) → concat_mlp(256) → [自注意力机制] → 6个策略头 + 价值头`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c47ac59",
   "metadata": {},
   "source": [
    "### **具体代码**\n",
    "\n",
    "在`conf/conf.py`中加入自注意力机制的参数：\n",
    "```python\n",
    "USE_SELF_ATTENTION = True   # 开关\n",
    "SA_TOKENS = 4               # 虚拟token数T\n",
    "SA_DIM = 64                 # 每个token维度D，需满足 T * D == 256\n",
    "SA_HEADS = 4                # Multi-Head\n",
    "SA_LAYERS = 2               # Transformer Encoder 层数\n",
    "SA_DROPOUT = 0.0            # 注意力/FFN dropout\n",
    "```\n",
    "\n",
    "模型初始化中加入自注意力机制结构：\n",
    "```python\n",
    "self.use_self_attn = bool(getattr(Config, \"USE_SELF_ATTENTION\", True))\n",
    "sa_tokens  = int(getattr(Config, \"SA_TOKENS\", 4))   # T\n",
    "sa_dim     = int(getattr(Config, \"SA_DIM\", 64))     # D\n",
    "sa_heads   = int(getattr(Config, \"SA_HEADS\", 4))\n",
    "sa_layers  = int(getattr(Config, \"SA_LAYERS\", 2))\n",
    "sa_dropout = float(getattr(Config, \"SA_DROPOUT\", 0.0))\n",
    "\n",
    "# 防止错误\n",
    "assert sa_tokens * sa_dim == 256, f\"Self-Attn shape mismatch: {sa_tokens} * {sa_dim} != 256\"\n",
    "\n",
    "# 将256维共享表征拆为T个token，每个D维\n",
    "self.attn_token_proj = nn.Linear(256, sa_tokens * sa_dim)\n",
    "\n",
    "# Transformer编码层\n",
    "encoder_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=sa_dim,\n",
    "    nhead=sa_heads,\n",
    "    dim_feedforward=sa_dim * 4,\n",
    "    dropout=sa_dropout,\n",
    "    activation=\"gelu\",\n",
    "    batch_first=True,\n",
    ")\n",
    "self.attn_encoder = nn.TransformerEncoder(encoder_layer, num_layers=sa_layers)\n",
    "\n",
    "# 将编码后的pooled表征映回256，并做LayerNorm残差融合\n",
    "self.attn_out_proj = nn.Linear(sa_dim, 256)\n",
    "self.attn_ln = nn.LayerNorm(256)\n",
    "```\n",
    "\n",
    "```python\n",
    "if self.use_self_attn:\n",
    "    B = fc_public_result.size(0)\n",
    "    # 256 → T*D → [B, T, D]\n",
    "    tokens = self.attn_token_proj(fc_public_result).view(B, -1, int(getattr(Config, \"SA_DIM\", 64)))\n",
    "    tokens = self.attn_encoder(tokens)             # [B, T, D]\n",
    "    pooled = tokens.mean(dim=1)                    # [B, D] 平均池化\n",
    "    attn_feat = self.attn_out_proj(pooled)         # [B, 256]\n",
    "    fc_public_result = self.attn_ln(fc_public_result + attn_feat)  # 残差+LN保稳\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae34ebaf",
   "metadata": {},
   "source": [
    "### **整体流程**\n",
    "\n",
    "\n",
    "#### **重要核心结构与关键配置**\n",
    "\n",
    "**模型类**：`Modek(nn.Module)`\n",
    "\n",
    "1. `concat_mlp`特征共享表征\n",
    "\n",
    "结构：两层mlp, [in_dim -> 256 -> 256]最后一层保留激活\n",
    "\n",
    "输入维度`in_dim`来自`DimConfig.DIM_OF_FEATURE[0]`\n",
    "\n",
    "2. 自注意力模块\n",
    "\n",
    "3. 多头策略分支\n",
    "\n",
    "6个分支与`Config.LABEL_SIZE_LIST`对齐，每个分支都是`MLP(256→256→label_dim_i)`，输出对应动作分量的`logits`\n",
    "\n",
    "4. 价值分支\n",
    "\n",
    "`MLP(256→256→1)`，输出状态价值`V(s)`，用于PPO的优势/价值损失\n",
    "\n",
    "**前向流程**：(记 batch 大小为 B，配置默认 T=4, D=64, T*D=256)\n",
    "\n",
    "\n",
    "1. 共享特征\n",
    "```python\n",
    "fc_public_result = self.concat_mlp(feature_vec)  # [B, 256]\n",
    "```\n",
    "\n",
    "2. 自注意力\n",
    "```python\n",
    "tokens   = self.attn_token_proj(fc_public_result)      # [B, 256]\n",
    "tokens   = tokens.view(B, T, D)                        # [B, 4, 64]\n",
    "tokens   = self.attn_encoder(tokens)                   # [B, 4, 64]\n",
    "pooled   = tokens.mean(dim=1)                          # [B, 64]\n",
    "attn_feat= self.attn_out_proj(pooled)                  # [B, 256]\n",
    "y        = self.attn_ln(fc_public_result + attn_feat)  # [B, 256]\n",
    "# 当不使用自注意力机制时，y=fc_public_result\n",
    "```\n",
    "\n",
    "3. 多头策略输出\n",
    "```python\n",
    "for each i in range(6):\n",
    "    logits_i = self.label_mlp[f\"hero_label{i}_mlp\"](y)   # [B, label_dim_i]\n",
    "```\n",
    "\n",
    "4. 价值输出\n",
    "```python\n",
    "value = self.value_mlp(y)   # [B, 1]\n",
    "```\n",
    "\n",
    "5. 返回值\n",
    "\n",
    "训练时：返回`[logits_0, …, logits_5, value]`\n",
    "推理时：把6个logits拼接成大向量，并且附上`value, lstm_cell_output, lstm_hidden_output`\n",
    "\n",
    "#### **参数设置**\n",
    "\n",
    "`T=4, D=64, heads=4, L=2, dropout=0.0`，出现震荡时可以`L=1`或`dropout=0.0`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8fdc2f",
   "metadata": {},
   "source": [
    "### **补充解释**\n",
    "\n",
    "1. 目前网络结构未真正使用LSTM(以后需要加入)，由于是对战类的游戏环境，需要加入LSTM以获取敌方的信息(过去若干步的隐藏状态信息)\n",
    "\n",
    "2. 当前策略输出是6个分支(heads)的logits列表，与`Config.LABEL_SIZE_LIST`一一对应，具体内容包括如下：\\\n",
    "    1. 动作类型(head0)12维\n",
    "    2. 参数化分支(head1-4)16维\n",
    "    3. 目标选择(head5)9维\n",
    "注意会配合合法动作掩码进行屏蔽非法动作\n",
    "\n",
    "3. 训练/评估的`forward`返回值\n",
    "\n",
    "- 训练模式：\n",
    "```python\n",
    "[\n",
    "  logits_head_0: [B, label_dim_0],\n",
    "  logits_head_1: [B, label_dim_1],\n",
    "  logits_head_2: [B, label_dim_2],\n",
    "  logits_head_3: [B, label_dim_3],\n",
    "  logits_head_4: [B, label_dim_4],\n",
    "  logits_head_5: [B, label_dim_5],\n",
    "  value:         [B, 1]\n",
    "]\n",
    "```\n",
    "\n",
    "- 评估模式：\n",
    "```python\n",
    "[\n",
    "  concat_logits: [B, sum(label_dim_i)],  # 把6个head的logits沿特征维拼起来\n",
    "  value:         [B, 1],\n",
    "  lstm_cell_out,                         # 占位(未真正完成设计)\n",
    "  lstm_hidden_out\n",
    "]\n",
    "```\n",
    "\n",
    "训练模式为多头返回，评估模式拼接返回。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55807e73",
   "metadata": {},
   "source": [
    "### **后序工作**\n",
    "\n",
    "1. 可以继续对网络结构进行优化，当前只加入了自注意力机制，简单的`token`划分，未针对具体的情况进行划分，同时可以考虑对`token`进行汇聚而不是直接使用平均池化\n",
    "\n",
    "2. 对于LSTM，后序在加入其他的特征之后需要启用，循环神经网络的记忆能力对对战类游戏非常有必要\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
